
\documentclass{dima}
          
\begin{document}

% ****************** TITLE ****************************************

\title{Data Analytics using {\ttlit KNIME} open source tool}

% ****************** AUTHORS **************************************

\numberofauthors{3}

\author{
% 1st. author
\alignauthor{DÃ¶rheit, Eric\\
       \affaddr{TU Berlin}\\
       \affaddr{Berlin, Germany}\\
       \email{eric.doerheit@campus.tu-berlin.de}}
% 2nd. author
\alignauthor{Bode, Olga\\
       \affaddr{TU Berlin}\\
       \affaddr{Berlin, Germany}\\
       \email{olga.bode@mailbox.tu-berlin.de}}
% 3rd. author
\alignauthor{Poljak, Dorothea\\
       \affaddr{TU Berlin}\\
       \affaddr{Berlin, Germany}\\
       \email{@mailbox.tu-berlin.de}}
}

\maketitle

% ****************** TEXT **************************************

\begin{abstract}
We present our results of the evaluation of the open source tool \textit{KNIME} which is used for data analytics and data mining. We choose anomaly detection as the subject to evaluate \textit{KNIME} as many methods of data analytics such as clustering, classification, time series analysis and statistical techniques are applicable to anomaly detection \cite{Chandola:2009:ADS:1541880.1541882}. As a data set for the analysis we use the data provided for the \textit{DEBS Grand Challenge 2012} \cite{Jerzak:2012:DGC:2335484.2335536}.
\end{abstract}

\section{Introduction}
For the evaluation of \textit{KNIME} we first investigated the tool by following several white-papers provided by \textit{KNIME}. We start with describing the functionalities and the usage of \textit{KNIME}. Then we explain the data used for the evaluation. Subsequently we provide an overview on anomaly detection based on \cite{Chandola:2009:ADS:1541880.1541882}. In Section~\ref{sec:ADwK} we evaluate \textit{KNIME} through applying anomaly detection on the data set described in Section~\ref{sec:D2012GC}.

\subsection{The Open Source Tool KNIME}
\label{sec:TOSTK}

In this section we give an overview on \textit{KNIME} based on three white-papers provided by \textit{KNIME}:
\begin{itemize}
\item Big Data, Smart Energy, and Predictive Analytics
\item Anomaly Detection in Predictive Maintenance
\item KNIME opens the Doors to Big Data
\end{itemize}

\textbf{TODOS:}
\begin{itemize}
\item General features of \textit{KNIME} $\rightarrow$ what can you do with \textit{KNIME} (ETL, Mining, Analysis, Visualization etc.)
\item How to use \textit{KNIME}? $\rightarrow$ Workflows, Nodes, ... (describe the usage of \textit{KNIME} in general)
\item Example workflow(s) based on the \textit{KNIME} white-papers
\item Overview on the nodes that are available and name, that there is an API to develop your own nodes
\item Tiny summary (2 sentences) if possible
\end{itemize}

\subsection{DEBS 2012 Grand Challenge}
\label{sec:D2012GC}

\textbf{TODOS:}
\begin{itemize}
\item Describe the challenge / the origin of the data
\item Explain the data set
\end{itemize}

\subsection{Anomaly Detection}
\label{sec:AD}

Anomalies in data are patterns which do not conform to the expected behavior and anomaly detections deals with finding this patterns \cite{Chandola:2009:ADS:1541880.1541882}. There are many techniques that can be applied to detect anomalies. Subsequently we describe classification based, nearest neighbor based and clustering based techniques as well as statistical anomaly detection techniques.

\subsubsection{Based on Classification}

\subsubsection{Based on Nearest Neighbor}

\subsubsection{Based on Clustering}
\label{sec:BoClus}

Clustering is a technique that assigns data instances to clusters. There are clustering algorithms which assign each data instance to a cluster and also algorithms which let data instances unassigned. In order to use a clustering as a means to detect anomalies, it is necessary to assume what an anomaly is in the context of clustering. There are three assumption usable for this purpose \cite{Chandola:2009:ADS:1541880.1541882}:
\begin{enumerate}
\item Normal data instances belong to clusters while anomalies do not.
\item Normal data instances can be found closest to a cluster centroid while anomalies are far away from their closest cluster centroid.
\item Normal data instances lie in dense clusters while anomalies are in small or sparse clusters.
\end{enumerate}
Assuming the first statement, algorithms such as DBSCAN, ROCK and SNN clustering are usable as they do not assign each data instance to a cluster. Based on the second assumption algorithms such as Self-Organized Maps (SOM), K-Means clustering as well as Expectation Maximization (EM) can be used due to the fact that they not only assign a data instance to a cluster but also compute its distance to its closest cluster centroid. Given the third assumption the use of various clustering algorithms is possible but it is needed to additionally compute the density/size of each cluster and define thresholds below which the data instances of these clusters are anomalies.

\subsubsection{Statistical Anomaly Detection Techniques}

\section{Anomaly Detection with KNIME}
\label{sec:ADwK}

In the following we describe how to detect anomal data instances in the \textit{DEBS 2012 Grand Challenge} data set using \textit{KNIME}. Therefore we start with explaining the handling of the large data set and proceed with the ETL process. Subsequently, we focus on the application of the techniques illustrated in Section~\ref{sec:AD}.

\subsection{Handling of large Data Sets}
\textit{KNIME} offers many possibilities to pull data out of different data sources as described in Section~\ref{sec:TOSTK}. Since \textit{KNIME} stores data either in-memory or on the local filesystem, it is not possible to directly take advantage of a a distributed filesystem such as \textit{HDFS}\footnote{Reference HDFS paper} within \textit{KNIME}. Hence, we need a way to handle large files.

\subsubsection{Split into smaller Files}
The first solution is simply splitting the large file into smaller ones, e.g. with a command-line tool such as \textit{split} or \textit{awk}, and then making use of the loop and file list nodes in \textit{KNIME} to iterate over the newly created files and execute the analytics on each of the files.

\subsubsection{Use of Databases}
The second option is to use a database to store the data set. Therefore, you can also split a large file into smaller ones, iterate over them and append a database table with the files data. An other way is not to use \textit{KNIME} and to directly import the file into the database. In our case we use a \textit{MySQL} database and the workflow shown in Figure~\ref{fig:WFMySQLImports}.\\
To import data from a database into \textit{KNIME} there are multiple nodes available to build SQL queries and execute them. We use a loop to import chunks of data so that we do not run out of memory as this decreases the performance of \textit{KNIME}.

%TODO: FIGURE MYSQL IMPORT

\subsubsection{Big Data Extensions}
An alternative for handling large files is to make use of the Big Data Extensions which \textit{KNIME} offers. They enable to connect to a \textit{HDFS}, where you can upload files to or read files from, and to \textit{Hive} as well as \textit{Impala}, which allows to execute SQL queries on an \textit{Hadoop} system. These extensions make it possible to access data from these sources but do not accelerate \textit{KNIME} in the favor of executing \textit{KNIME} workflows in a cluster.

\subsection{The ETL Process with KNIME}
ETL describes the process of extracting, transforming and loading data. In the extractions phase data is extracted from various data sources. In our case we already have one homogeneous file. Then the transformation phase follows where the data is altered in order to be able to run queries and do analysis. Therefore, we split the timestamp into multiple columns and filter columns which remain constant. Afterwards we store the new tables into a \textit{MySQL} database.

% TODO: INSERT AND EXPLAIN ETL WORKFLOW

\subsection{Anomaly Detection Techniques}
\subsubsection{Classification}

\subsubsection{Clustering}
In Section~\ref{sec:BoClus} we describe multiple approaches to detect anomalies with clustering. We apply these techniques on the data explained in Section~\ref{sec:D2012GC}.

\textbf{TODOS:}
\begin{itemize}
\item How can you do clustering with KNIME? (Explain available nodes)
\item We use the columns bla bla bla of the data set (see Table~\ref{tab:bla})
\item Three ideas of anomalies with clustering --> Which nodes can be used?
\item Results of clustering nodes
\item Explain where the anomalies are / Compute anomalies (threshold for distance to centroid)
\item Describe how to visualize 
\end{itemize}

\subsubsection{Statistical Anomaly Detection Techniques}

\textbf{TODOS:}
\begin{itemize}
\item How to input the big files into \textit{KNIME}? $\rightarrow$ Split files and iterate over them, input into MySQL etc.
\item Describe the ETL process with \textit{KNIME} of the data
\item Describe how to do anomaly detection with \textit{KNIME}
\end{itemize}

\section{Results}
\textbf{TODOS:}
\begin{itemize}
\item \textit{KNIME} is not directly suitable for Big Data Processing
\item Easy tool to do advanced data analytics without deep knowledge of underlying algorithms and math
\item Enables users which are no data scientists or have a strong background in this field to do data analysis
\item Good integration with various other tools (R, Weka) and adoptable to own needs with Java, Python, ... snippet nodes and the API to create own nodes
\item Relatively slow
\end{itemize}


\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{sigproc}

\end{document}
